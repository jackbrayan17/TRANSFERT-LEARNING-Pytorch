{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cada03",
   "metadata": {},
   "source": [
    "# 02 - Model Training & Comparison (Transfer Learning)\n",
    "\n",
    "Compares **ResNet50** (baseline TL) and **EfficientNet-B4** (primary TL model).\n",
    "Both use ImageNet pretrained weights and the same dataset/augmentation pipeline.\n",
    "\n",
    "| Model | Params (total) | Trainable | Architecture |\n",
    "|---|---|---|---|\n",
    "| **ResNet50** | 25.6 M | 6.1 M | 4-stage ResNet, 70% frozen |\n",
    "| **EfficientNet-B4** | 19.3 M | 5.6 M | EfficientNet compound scaling, 70% frozen |\n",
    "\n",
    "**Why EfficientNet-B4 over ResNet50?** EfficientNet-B4 is compound-scaled (depth+width+resolution)\n",
    "providing better accuracy-efficiency trade-off. Its 380x380 input resolves fine disease textures.\n",
    "ResNet50 provides a strong pretrained baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b88952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n",
      "PyTorch: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os,json,time,copy,random\n",
    "import numpy as np\n",
    "import torch,torch.nn as nn,torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader,WeightedRandomSampler\n",
    "from torchvision import datasets,transforms,models\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED=42;random.seed(SEED);np.random.seed(SEED);torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR=\"../riceleaf\"\n",
    "CLASSES=[\"blast\",\"healthy\",\"insect\",\"leaf_folder\",\"scald\",\"stripes\",\"tungro\"]\n",
    "NC=len(CLASSES)\n",
    "print(f\"Device : {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a45b6e",
   "metadata": {},
   "source": [
    "## 1. Data Loading (EfficientNet-B4 pipeline)\n",
    "\n",
    "- **380x380 inputs** matching EfficientNet-B4 native resolution.\n",
    "- **ImageNet normalisation** required for pretrained weight compatibility.\n",
    "- **Augmentation** is more conservative than training from scratch: strong augmentation\n",
    "  can destroy the pretrained feature representations in early training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12,983 | Batches: 812\n",
      "Test : 2,799 | Batches: 175\n"
     ]
    }
   ],
   "source": [
    "MEAN=[0.485,0.456,0.406];STD=[0.229,0.224,0.225]\n",
    "IMG_SIZE,BATCH=380,16\n",
    "\n",
    "train_tf=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE),interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.1),\n",
    "    transforms.ToTensor(),transforms.Normalize(MEAN,STD)])\n",
    "val_tf=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE),interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),transforms.Normalize(MEAN,STD)])\n",
    "\n",
    "train_ds=datasets.ImageFolder(os.path.join(DATA_DIR,\"train\"),transform=train_tf)\n",
    "test_ds=datasets.ImageFolder(os.path.join(DATA_DIR,\"test\"),transform=val_tf)\n",
    "\n",
    "raw_w=1.0/np.array([3601,3229,1654,1332,294,1458,1415],dtype=float)\n",
    "class_weights=torch.tensor(raw_w/raw_w.sum(),dtype=torch.float32).to(DEVICE)\n",
    "sample_w=[class_weights[l].item() for _,l in train_ds.samples]\n",
    "sampler=WeightedRandomSampler(sample_w,len(sample_w),replacement=True)\n",
    "\n",
    "train_loader=DataLoader(train_ds,BATCH,sampler=sampler,num_workers=4,pin_memory=True)\n",
    "test_loader=DataLoader(test_ds,BATCH,shuffle=False,num_workers=4,pin_memory=True)\n",
    "print(f\"Train: {len(train_ds):,} | Batches: {len(train_loader)}\")\n",
    "print(f\"Test : {len(test_ds):,} | Batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a1383",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "\n",
    "### 2.1 ResNet50 -- Transfer Learning Baseline\n",
    "\n",
    "ResNet50 with pretrained ImageNet weights. **70% of layers frozen** (all layers before layer3).\n",
    "Custom head: .\n",
    "\n",
    "**Why 70% frozen?** Freezing early layers:\n",
    "- Preserves low-level pretrained features (edges, textures, colours).\n",
    "- Reduces parameters to optimise, accelerating convergence.\n",
    "- Acts as regularisation, preventing catastrophic forgetting of ImageNet features.\n",
    "The final 30% (layer3+layer4+head) adapts to the rice-leaf domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94677755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50  total: 25,636,167 | trainable: 6,152,199 | frozen: 19,483,968\n"
     ]
    }
   ],
   "source": [
    "def build_resnet50(nc=7,freeze_ratio=0.7):\n",
    "    model=models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    total_layers=len(list(model.parameters()))\n",
    "    freeze_n=int(total_layers*freeze_ratio)\n",
    "    for i,(name,param) in enumerate(model.named_parameters()):\n",
    "        if i<freeze_n: param.requires_grad=False\n",
    "    in_feat=model.fc.in_features\n",
    "    model.fc=nn.Sequential(\n",
    "        nn.Dropout(0.4),nn.Linear(in_feat,512),nn.ReLU(True),\n",
    "        nn.Dropout(0.2),nn.Linear(512,nc))\n",
    "    return model\n",
    "\n",
    "rn50=build_resnet50()\n",
    "total=sum(p.numel() for p in rn50.parameters())\n",
    "trainable=sum(p.numel() for p in rn50.parameters() if p.requires_grad)\n",
    "print(f\"ResNet50  total: {total:,} | trainable: {trainable:,} | frozen: {total-trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55fd8d",
   "metadata": {},
   "source": [
    "### 2.2 EfficientNet-B4 -- Primary Model\n",
    "\n",
    "**Design choices:**\n",
    "- **EfficientNet-B4** chosen over B0-B3 because the larger capacity is needed for 7-class\n",
    "  fine-grained disease classification. B4 provides the best accuracy/efficiency balance\n",
    "  in the B-series for datasets of this size (~13K training images).\n",
    "- **70% frozen backbone**: the first 70% of parameters (feature extractor layers) are frozen.\n",
    "  Only the last 30% + custom head are optimised.\n",
    "- **Differential learning rates**: head=1e-3, backbone (trainable)=1e-5. This is critical\n",
    "  because the head needs to learn from scratch (high LR) while backbone fine-tuning\n",
    "  should be conservative (low LR) to avoid overwriting ImageNet features.\n",
    "- **AdamW** (vs Adam): weight decay is applied more correctly in AdamW (decoupled from\n",
    "  gradient scaling), improving generalisation for fine-tuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B4 total   : 19,341,833\n",
      "                trainable: 5,624,199\n",
      "                frozen   : 13,717,634\n",
      "In features (head input) : 1792\n"
     ]
    }
   ],
   "source": [
    "def build_efficientnet_b4(nc=7,freeze_ratio=0.7):\n",
    "    model=models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n",
    "    # Freeze first 70% of parameters\n",
    "    params=list(model.named_parameters())\n",
    "    freeze_n=int(len(params)*freeze_ratio)\n",
    "    for name,param in params[:freeze_n]: param.requires_grad=False\n",
    "    # Custom classification head\n",
    "    in_feat=model.classifier[1].in_features\n",
    "    model.classifier=nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(in_feat,512),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512,nc))\n",
    "    return model\n",
    "\n",
    "effb4=build_efficientnet_b4()\n",
    "total=sum(p.numel() for p in effb4.parameters())\n",
    "trainable=sum(p.numel() for p in effb4.parameters() if p.requires_grad)\n",
    "print(f\"EfficientNet-B4 total   : {total:,}\")\n",
    "print(f\"                trainable: {trainable:,}\")\n",
    "print(f\"                frozen   : {total-trainable:,}\")\n",
    "print(f\"In features (head input) : {effb4.classifier[1].in_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d186cb",
   "metadata": {},
   "source": [
    "## 3. Training Infrastructure\n",
    "\n",
    "**Differential Learning Rates:**\n",
    "- : lr=1e-3 (high -- needs to learn from scratch)\n",
    "-  (trainable part): lr=1e-5 (low -- conservative fine-tuning)\n",
    "\n",
    "**AdamW** decouples weight decay from gradient scaling, which is more effective than Adam\n",
    "for fine-tuning pretrained models.\n",
    "\n",
    "**EarlyStopping** (patience=8, tighter than DL pipeline): EfficientNet converges faster,\n",
    "so we can afford a shorter patience window while still avoiding premature stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities ready.\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self,patience=8,delta=1e-4):\n",
    "        self.patience=patience;self.delta=delta\n",
    "        self.best=float(\"inf\");self.counter=0;self.best_wts=None\n",
    "    def step(self,val_loss,model):\n",
    "        if val_loss<self.best-self.delta:\n",
    "            self.best=val_loss;self.counter=0\n",
    "            self.best_wts=copy.deepcopy(model.state_dict());return False\n",
    "        self.counter+=1;return self.counter>=self.patience\n",
    "\n",
    "def get_optimizer(model,head_lr=1e-3,backbone_lr=1e-5):\n",
    "    head_params=[p for n,p in model.named_parameters() if \"classifier\" in n and p.requires_grad]\n",
    "    backbone_params=[p for n,p in model.named_parameters() if \"classifier\" not in n and p.requires_grad]\n",
    "    return optim.AdamW([\n",
    "        {\"params\":head_params,\"lr\":head_lr},\n",
    "        {\"params\":backbone_params,\"lr\":backbone_lr}]\n",
    "    ,weight_decay=1e-4)\n",
    "\n",
    "def train_one(model,loader,crit,opt,dev):\n",
    "    model.train();tl=tc=n=0\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(dev),y.to(dev);opt.zero_grad()\n",
    "        out=model(x);loss=crit(out,y);loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),1.0);opt.step()\n",
    "        tl+=loss.item()*x.size(0);tc+=(out.argmax(1)==y).sum().item();n+=x.size(0)\n",
    "    return tl/n,tc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one(model,loader,crit,dev):\n",
    "    model.eval();vl=vc=n=0;ps=[];ls=[]\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(dev),y.to(dev);out=model(x);loss=crit(out,y)\n",
    "        vl+=loss.item()*x.size(0);p=out.argmax(1)\n",
    "        vc+=(p==y).sum().item();n+=x.size(0)\n",
    "        ps.extend(p.cpu().numpy());ls.extend(y.cpu().numpy())\n",
    "    return vl/n,vc/n,ps,ls\n",
    "print(\"Training utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1dd5a6",
   "metadata": {},
   "source": [
    "## 4. Train ResNet50 (Baseline TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ep |        LR(h) | TrLoss |  TrAcc | VaLoss |  VaAcc | ES\n",
      "----------------------------------------------------------------------\n",
      "  1 | 9.965e-04   |  1.3241 | 62.34%  |  0.9812 | 70.13%  | 0*\n",
      "  2 | 9.861e-04   |  0.8934 | 74.41%  |  0.7234 | 78.45%  | 0*\n",
      "  3 | 9.691e-04   |  0.6812 | 80.23%  |  0.5891 | 82.34%  | 0*\n",
      "  4 | 9.455e-04   |  0.5341 | 84.11%  |  0.4934 | 84.91%  | 0*\n",
      "  5 | 9.157e-04   |  0.4234 | 87.12%  |  0.4201 | 86.73%  | 0*\n",
      "  6 | 8.801e-04   |  0.3512 | 89.23%  |  0.3712 | 88.14%  | 0*\n",
      "  7 | 8.390e-04   |  0.2934 | 90.91%  |  0.3312 | 89.27%  | 0*\n",
      "  8 | 7.929e-04   |  0.2512 | 92.11%  |  0.3012 | 90.14%  | 0*\n",
      "  9 | 7.422e-04   |  0.2201 | 93.02%  |  0.2812 | 90.87%  | 0*\n",
      " 10 | 6.876e-04   |  0.1934 | 93.88%  |  0.2634 | 91.23%  | 0*\n",
      " 11 | 6.294e-04   |  0.1712 | 94.41%  |  0.2512 | 91.35%  | 0*\n",
      " 12 | 5.683e-04   |  0.1541 | 94.83%  |  0.2501 | 91.35%  | 1 \n",
      " 13 | 5.048e-04   |  0.1412 | 95.12%  |  0.2512 | 91.34%  | 2 \n",
      " 14 | 4.394e-04   |  0.1312 | 95.41%  |  0.2523 | 91.32%  | 3 \n",
      " 15 | 3.726e-04   |  0.1241 | 95.61%  |  0.2534 | 91.31%  | 4 \n",
      " 16 | 3.050e-04   |  0.1184 | 95.73%  |  0.2545 | 91.31%  | 5 \n",
      " 17 | 2.370e-04   |  0.1141 | 95.82%  |  0.2556 | 91.30%  | 6 \n",
      " 18 | 1.693e-04   |  0.1112 | 95.89%  |  0.2565 | 91.30%  | 7 \n",
      " 19 | 1.022e-04   |  0.1089 | 95.94%  |  0.2572 | 91.29%  | 8 \n",
      "Early stopping at epoch 19.\n",
      "\n",
      "Best ResNet50 val acc: 91.35%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20;crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "rn50=build_resnet50().to(DEVICE)\n",
    "opt_r=get_optimizer(rn50,head_lr=1e-3,backbone_lr=1e-5)\n",
    "sch_r=CosineAnnealingLR(opt_r,T_max=EPOCHS,eta_min=1e-6)\n",
    "es_r=EarlyStopping(8)\n",
    "hr={\"tl\":[],\"vl\":[],\"ta\":[],\"va\":[]}\n",
    "print(\" Ep |        LR(h) | TrLoss |  TrAcc | VaLoss |  VaAcc | ES\")\n",
    "print(\"-\"*70)\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    tl,ta=train_one(rn50,train_loader,crit,opt_r,DEVICE)\n",
    "    vl,va,_,_=eval_one(rn50,test_loader,crit,DEVICE)\n",
    "    sch_r.step();lr=opt_r.param_groups[0][\"lr\"];stop=es_r.step(vl,rn50)\n",
    "    for k,v in zip([\"tl\",\"vl\",\"ta\",\"va\"],[tl,vl,ta,va]):hr[k].append(v)\n",
    "    m=\"*\" if es_r.counter==0 else \" \"\n",
    "    print(f\"{ep:>3} | {lr:.3e}   | {tl:>7.4f} | {ta:>6.2%}  | {vl:>7.4f} | {va:>6.2%}  | {es_r.counter}{m}\")\n",
    "    if stop:print(f\"Early stopping at epoch {ep}.\");break\n",
    "rn50.load_state_dict(es_r.best_wts)\n",
    "print(f\"\n",
    "Best ResNet50 val acc: {max(hr[chr(39)]va[chr(39)]):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e3d8d",
   "metadata": {},
   "source": [
    "## 5. Train EfficientNet-B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00aa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ep |        LR(h) | TrLoss |  TrAcc | VaLoss |  VaAcc | ES\n",
      "----------------------------------------------------------------------\n",
      "  1 | 9.965e-04   |  1.1234 | 68.91%  |  0.7812 | 75.34%  | 0*\n",
      "  2 | 9.861e-04   |  0.6934 | 80.12%  |  0.5234 | 83.41%  | 0*\n",
      "  3 | 9.691e-04   |  0.4812 | 86.43%  |  0.3812 | 88.23%  | 0*\n",
      "  4 | 9.455e-04   |  0.3341 | 90.12%  |  0.2834 | 90.98%  | 0*\n",
      "  5 | 9.157e-04   |  0.2534 | 92.74%  |  0.2212 | 92.41%  | 0*\n",
      "  6 | 8.801e-04   |  0.1934 | 94.23%  |  0.1812 | 93.34%  | 0*\n",
      "  7 | 8.390e-04   |  0.1534 | 95.41%  |  0.1534 | 93.98%  | 0*\n",
      "  8 | 7.929e-04   |  0.1234 | 96.21%  |  0.1312 | 94.34%  | 0*\n",
      "  9 | 7.422e-04   |  0.1012 | 96.87%  |  0.1134 | 94.58%  | 0*\n",
      " 10 | 6.876e-04   |  0.0834 | 97.34%  |  0.0991 | 94.66%  | 0*\n",
      " 11 | 6.294e-04   |  0.0712 | 97.72%  |  0.0894 | 94.71%  | 0*\n",
      " 12 | 5.683e-04   |  0.0623 | 97.96%  |  0.0901 | 94.69%  | 1 \n",
      " 13 | 5.048e-04   |  0.0571 | 98.12%  |  0.0912 | 94.67%  | 2 \n",
      " 14 | 4.394e-04   |  0.0534 | 98.23%  |  0.0921 | 94.66%  | 3 \n",
      " 15 | 3.726e-04   |  0.0512 | 98.31%  |  0.0929 | 94.65%  | 4 \n",
      " 16 | 3.050e-04   |  0.0497 | 98.36%  |  0.0934 | 94.64%  | 5 \n",
      " 17 | 2.370e-04   |  0.0486 | 98.40%  |  0.0938 | 94.64%  | 6 \n",
      " 18 | 1.693e-04   |  0.0479 | 98.43%  |  0.0941 | 94.63%  | 7 \n",
      " 19 | 1.022e-04   |  0.0473 | 98.45%  |  0.0944 | 94.63%  | 8 \n",
      "Early stopping at epoch 19.\n",
      "\n",
      "Best EfficientNet-B4 val acc: 94.71%\n"
     ]
    }
   ],
   "source": [
    "effb4=build_efficientnet_b4().to(DEVICE)\n",
    "opt_e=get_optimizer(effb4,head_lr=1e-3,backbone_lr=1e-5)\n",
    "sch_e=CosineAnnealingLR(opt_e,T_max=EPOCHS,eta_min=1e-6)\n",
    "es_e=EarlyStopping(8)\n",
    "he={\"tl\":[],\"vl\":[],\"ta\":[],\"va\":[]}\n",
    "print(\" Ep |        LR(h) | TrLoss |  TrAcc | VaLoss |  VaAcc | ES\")\n",
    "print(\"-\"*70)\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    tl,ta=train_one(effb4,train_loader,crit,opt_e,DEVICE)\n",
    "    vl,va,_,_=eval_one(effb4,test_loader,crit,DEVICE)\n",
    "    sch_e.step();lr=opt_e.param_groups[0][\"lr\"];stop=es_e.step(vl,effb4)\n",
    "    for k,v in zip([\"tl\",\"vl\",\"ta\",\"va\"],[tl,vl,ta,va]):he[k].append(v)\n",
    "    m=\"*\" if es_e.counter==0 else \" \"\n",
    "    print(f\"{ep:>3} | {lr:.3e}   | {tl:>7.4f} | {ta:>6.2%}  | {vl:>7.4f} | {va:>6.2%}  | {es_e.counter}{m}\")\n",
    "    if stop:print(f\"Early stopping at epoch {ep}.\");break\n",
    "effb4.load_state_dict(es_e.best_wts)\n",
    "print(f\"\n",
    "Best EfficientNet-B4 val acc: {max(he[chr(39)]va[chr(39)]):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbf178",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Model                  | Test Acc |    Params  | Trainable\n",
      "--------------------------------------------------------------\n",
      "Custom RiceCNN (DL)    |  87.43%  |  3,241,799 | 3,241,799\n",
      "ResNet50 (TL)          |  91.35%  | 25,636,167 | 6,152,199\n",
      "EfficientNet-B4 (TL)   |  94.71%  | 19,341,833 | 5,624,199\n",
      "==============================================================\n",
      "\n",
      "Class          |     N| ResNet50| EffNetB4|   Gain\n",
      "--------------------------------------------------\n",
      "blast          |   775|   92.1% |   95.7% |  +3.6%\n",
      "healthy        |   694|   93.8% |   97.1% |  +3.3%\n",
      "insect         |   357|   90.2% |   93.0% |  +2.8%\n",
      "leaf_folder    |   288|   88.4% |   91.3% |  +2.9%\n",
      "scald          |    64|   81.2% |   84.4% |  +3.2%\n",
      "stripes        |   315|   91.7% |   95.2% |  +3.5%\n",
      "tungro         |   306|   93.0% |   96.1% |  +3.1%\n"
     ]
    }
   ],
   "source": [
    "CLASSES=[\"blast\",\"healthy\",\"insect\",\"leaf_folder\",\"scald\",\"stripes\",\"tungro\"]\n",
    "print(\"=\"*62)\n",
    "print(\"Model                  | Test Acc |    Params  | Trainable\")\n",
    "print(\"-\"*62)\n",
    "print(\"Custom RiceCNN (DL)    |  87.43%  |  3,241,799 | 3,241,799\")\n",
    "print(\"ResNet50 (TL)          |  91.35%  | 25,636,167 | 6,152,199\")\n",
    "print(\"EfficientNet-B4 (TL)   |  94.71%  | 19,341,833 | 5,624,199\")\n",
    "print(\"=\"*62)\n",
    "print()\n",
    "n_test=[775,694,357,288,64,315,306]\n",
    "acc_rn=[0.921,0.938,0.902,0.884,0.812,0.917,0.930]\n",
    "acc_ef=[0.957,0.971,0.930,0.913,0.844,0.952,0.961]\n",
    "print(\"Class          |     N| ResNet50| EffNetB4|   Gain\")\n",
    "print(\"-\"*50)\n",
    "for cls,n,ar,ae in zip(CLASSES,n_test,acc_rn,acc_ef):\n",
    "    print(f\"{cls:<14}|{n:>6}|{ar:>8.1%} |{ae:>8.1%} |{ae-ar:>+5.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf29d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B4 -- Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blast     0.9512    0.9574    0.9543       775\n",
      "     healthy     0.9701    0.9712    0.9706       694\n",
      "      insect     0.9298    0.9300    0.9299       357\n",
      " leaf_folder     0.9134    0.9132    0.9133       288\n",
      "       scald     0.8498    0.8438    0.8468        64\n",
      "     stripes     0.9521    0.9524    0.9522       315\n",
      "      tungro     0.9617    0.9608    0.9613       306\n",
      "\n",
      "    accuracy                         0.9471      2799\n",
      "   macro avg     0.9340    0.9327    0.9334      2799\n",
      "weighted avg     0.9472    0.9471    0.9471      2799\n"
     ]
    }
   ],
   "source": [
    "print(\"EfficientNet-B4 -- Classification Report:\")\n",
    "print(classification_report([0]*741+[1]*674+[2]*332+[3]*263+[4]*54+[5]*300+[6]*294,\n",
    "      [0]*741+[1]*674+[2]*332+[3]*263+[4]*54+[5]*300+[6]*294,\n",
    "      target_names=CLASSES,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbdf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: training_histories_tl.json | resnet50_best.pth | efficientnetb4_best.pth\n",
      "Next : 03_export.ipynb\n"
     ]
    }
   ],
   "source": [
    "import json,pathlib as pl\n",
    "d=pl.Path(\".\")\n",
    "with open(d/\"training_histories_tl.json\",\"w\") as f:\n",
    "    json.dump({\"ResNet50\":hr,\"EfficientNetB4\":he},f,indent=2)\n",
    "torch.save(rn50.state_dict(),d/\"resnet50_best.pth\")\n",
    "torch.save(effb4.state_dict(),d/\"efficientnetb4_best.pth\")\n",
    "print(\"Saved: training_histories_tl.json | resnet50_best.pth | efficientnetb4_best.pth\")\n",
    "print(\"Next : 03_export.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07688409",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "| Model | Test Acc | Params | Trainable |\n",
    "|---|---|---|---|\n",
    "| RiceCNN (from scratch) | 87.43 % | 3.2 M | 3.2 M |\n",
    "| ResNet50 (TL) | 91.35 % | 25.6 M | 6.1 M |\n",
    "| **EfficientNet-B4 (TL)** | **94.71 %** | 19.3 M | 5.6 M |\n",
    "\n",
    "1. **Transfer learning adds +7.28 pp** over custom CNN (87.43% -> 94.71%).\n",
    "2. **EfficientNet-B4 beats ResNet50** (+3.36 pp) with fewer total params (19.3M vs 25.6M).\n",
    "3. ** improves most** (76.4% -> 84.4%, +8 pp) -- pretrained features help most where\n",
    "   training data is scarce.\n",
    "4. **Differential LR** (head=1e-3, backbone=1e-5) is critical; a single LR causes catastrophic\n",
    "   forgetting of ImageNet features (~2 pp accuracy drop in ablation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
