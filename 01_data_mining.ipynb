{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f7b8a4",
   "metadata": {},
   "source": [
    "# 01 - Data Mining & Exploration (Transfer Learning)\n",
    "\n",
    "This notebook performs exploratory data analysis for the Transfer Learning pipeline.\n",
    "It mirrors  but focuses on considerations specific to\n",
    "fine-tuning a pretrained EfficientNet-B4:\n",
    "\n",
    "- **Input resolution**: EfficientNet-B4 was designed for **380x380** inputs (vs 224x224 for custom CNN).\n",
    "- **Normalisation**: We switch from dataset-specific stats to **ImageNet mean/std** because\n",
    "  pretrained weights learned features relative to ImageNet distribution.\n",
    "- **Data augmentation**: More conservative than training from scratch -- strong augmentation\n",
    "  can corrupt the pretrained feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../riceleaf\n",
      "Classes (7): [\"blast\", \"healthy\", \"insect\", \"leaf_folder\", \"scald\", \"stripes\", \"tungro\"]\n"
     ]
    }
   ],
   "source": [
    "import os,json,collections,pathlib as pl\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR=\"../riceleaf\"\n",
    "CLASSES=[\"blast\",\"healthy\",\"insect\",\"leaf_folder\",\"scald\",\"stripes\",\"tungro\"]\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Classes ({len(CLASSES)}): {CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1d0ca",
   "metadata": {},
   "source": [
    "## 1. Dataset Statistics\n",
    "\n",
    "### Why We Reuse the Same Dataset Split\n",
    "The train/test split from  (DL) is reused unchanged.\n",
    "This ensures a **fair comparison** between custom CNN (87.43%) and EfficientNet-B4 (94.71%).\n",
    "Using the same test set guarantees the accuracy difference is attributable to the model, not the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN SET (12,983 images):\n",
      "  blast          3601  ##############################\n",
      "  healthy        3229  ##########################\n",
      "  insect         1654  #############\n",
      "  leaf_folder    1332  ###########\n",
      "  scald           294  ##\n",
      "  stripes        1458  ############\n",
      "  tungro         1415  ###########\n",
      "\n",
      "TEST SET (2,799 images):\n",
      "  blast           775  ##############################\n",
      "  healthy         694  ##########################\n",
      "  insect          357  #############\n",
      "  leaf_folder     288  ###########\n",
      "  scald            64  ##\n",
      "  stripes         315  ############\n",
      "  tungro          306  ###########\n",
      "\n",
      "Total: 15,782 | Train: 12,983 (82%) | Test: 2,799 (18%)\n",
      "Imbalance ratio: 12.2x (blast:scald)\n"
     ]
    }
   ],
   "source": [
    "split_counts={\n",
    "    \"train\":{\"blast\":3601,\"healthy\":3229,\"insect\":1654,\"leaf_folder\":1332,\"scald\":294,\"stripes\":1458,\"tungro\":1415},\n",
    "    \"test\": {\"blast\":775, \"healthy\":694, \"insect\":357, \"leaf_folder\":288, \"scald\":64, \"stripes\":315, \"tungro\":306}\n",
    "}\n",
    "\n",
    "for split,counts in split_counts.items():\n",
    "    total=sum(counts.values())\n",
    "    print(f\"\n",
    "{split.upper()} SET ({total:,} images):\")\n",
    "    for cls,n in counts.items():\n",
    "        bar=\"#\"*int(n/max(counts.values())*30)\n",
    "        print(f\"  {cls:<14} {n:>4}  {bar}\")\n",
    "print()\n",
    "train_n=sum(split_counts[\"train\"].values())\n",
    "test_n=sum(split_counts[\"test\"].values())\n",
    "print(f\"Total: {train_n+test_n:,} | Train: {train_n:,} ({train_n/(train_n+test_n):.0%}) | Test: {test_n:,} ({test_n/(train_n+test_n):.0%})\")\n",
    "print(f\"Imbalance ratio: {max(split_counts[\"train\"].values())/min(split_counts[\"train\"].values()):.1f}x (blast:scald)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be547d",
   "metadata": {},
   "source": [
    "## 2. Normalisation: ImageNet vs Dataset Statistics\n",
    "\n",
    "**For Transfer Learning, ImageNet normalisation is mandatory:**\n",
    "EfficientNet-B4 pretrained weights learned features relative to ImageNet-normalised inputs.\n",
    "Using dataset-specific statistics would shift all pixel values out of the range the\n",
    "network was calibrated for, effectively invalidating the pretrained features.\n",
    "\n",
    "| Channel | ImageNet Mean | Dataset Mean | Difference |\n",
    "|---|---|---|---|\n",
    "| R | 0.485 | 0.8835 | +0.3985 |\n",
    "| G | 0.456 | 0.8862 | +0.4302 |\n",
    "| B | 0.406 | 0.8480 | +0.4420 |\n",
    "\n",
    "Rice leaf images are ~0.4 brighter than the average ImageNet image, but we still use\n",
    "ImageNet stats -- the pretrained convolutional filters adapt to this brightness offset\n",
    "through the frozen batch normalisation layers of the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdc7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel | ImageNet Mean | Dataset Mean | Offset\n",
      "----------------------------------------------------\n",
      "R       |        0.4850 |       0.8835 | +0.3985\n",
      "G       |        0.4560 |       0.8862 | +0.4302\n",
      "B       |        0.4060 |       0.8480 | +0.4420\n"
     ]
    }
   ],
   "source": [
    "imagenet_mean=[0.485,0.456,0.406]\n",
    "imagenet_std=[0.229,0.224,0.225]\n",
    "dataset_mean=[0.8835,0.8862,0.8480]\n",
    "dataset_std=[0.2158,0.2074,0.2905]\n",
    "\n",
    "print(\"Channel | ImageNet Mean | Dataset Mean | Offset\")\n",
    "print(\"-\"*52)\n",
    "for ch,im,dm in zip([\"R\",\"G\",\"B\"],imagenet_mean,dataset_mean):\n",
    "    print(f\"{ch:<8}|{im:>14.4f} |{dm:>13.4f} |{dm-im:>+8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f48ec",
   "metadata": {},
   "source": [
    "## 3. Input Resolution Analysis\n",
    "\n",
    "EfficientNet-B4 was designed for **380x380** inputs (EfficientNet compound scaling increases\n",
    "resolution with depth and width).\n",
    "\n",
    "Compared to the custom CNN (224x224):\n",
    "- Image area: 380x380 vs 224x224 = **2.9x larger** per image\n",
    "- Batch size must be halved (32->16) to fit GPU memory\n",
    "- Training time per epoch ~2.8x longer\n",
    "\n",
    "This is the trade-off for accessing pretrained features: higher quality, higher compute cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL train transforms:\n",
      "  Resize\n",
      "  RandomHorizontalFlip\n",
      "  RandomVerticalFlip\n",
      "  RandomRotation\n",
      "  ColorJitter\n",
      "  ToTensor\n",
      "  Normalize\n",
      "\n",
      "Input shape: torch.Size([1, 3, 380, 380])\n",
      "Memory/img : 1672.6 KB\n",
      "Batch=16   : 27.8 MB\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "import torch\n",
    "\n",
    "IMG_SIZE_TL=380\n",
    "MEAN_IN=[0.485,0.456,0.406];STD_IN=[0.229,0.224,0.225]\n",
    "\n",
    "tl_train_tf=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE_TL,IMG_SIZE_TL),interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.1),\n",
    "    transforms.ToTensor(),transforms.Normalize(MEAN_IN,STD_IN)])\n",
    "tl_val_tf=transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE_TL,IMG_SIZE_TL),interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),transforms.Normalize(MEAN_IN,STD_IN)])\n",
    "\n",
    "print(\"TL train transforms:\")\n",
    "for t in tl_train_tf.transforms: print(f\"  {t.__class__.__name__}\")\n",
    "print()\n",
    "dummy=torch.randn(1,3,380,380)\n",
    "print(f\"Input shape: {dummy.shape}\")\n",
    "print(f\"Memory/img : {dummy.nbytes/1024:.1f} KB\")\n",
    "print(f\"Batch=16   : {dummy.nbytes*16/1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a93c8",
   "metadata": {},
   "source": [
    "## 4. Class Weight Computation for Transfer Learning\n",
    "\n",
    "The same inverse-frequency weighting strategy is used as in the DL pipeline.\n",
    "The class weight for  is ~12x higher than for , directing the loss\n",
    "function to penalise scald misclassifications proportionally more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87589a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights for CrossEntropyLoss:\n",
      "------------------------------------------\n",
      "blast          w=0.0193 n=3601 |\n",
      "healthy        w=0.0215 n=3229 |\n",
      "insect         w=0.0421 n=1654 ||\n",
      "leaf_folder    w=0.0523 n=1332 |||\n",
      "scald          w=0.2370 n= 294 ||||||||||||||||||||\n",
      "stripes        w=0.0478 n=1458 ||\n",
      "tungro         w=0.0493 n=1415 |||\n",
      "\n",
      "scald weight / blast weight = 12.3x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_counts=np.array([3601,3229,1654,1332,294,1458,1415],dtype=float)\n",
    "raw_w=1.0/train_counts\n",
    "cw=raw_w/raw_w.sum()\n",
    "CLASSES=[\"blast\",\"healthy\",\"insect\",\"leaf_folder\",\"scald\",\"stripes\",\"tungro\"]\n",
    "print(\"Class weights for CrossEntropyLoss:\")\n",
    "print(\"-\"*42)\n",
    "for cls,w,n in zip(CLASSES,cw,train_counts.astype(int)):\n",
    "    bar=\"|\"*int(w/cw.max()*20)\n",
    "    print(f\"{cls:<14} w={w:.4f} n={n:>4} {bar}\")\n",
    "print()\n",
    "print(f\"scald weight / blast weight = {cw[4]/cw[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9d17b",
   "metadata": {},
   "source": [
    "## 5. Dataset Inspection -- Sample Quality Check\n",
    "\n",
    "Visual inspection of samples is critical before fine-tuning. Key observations:\n",
    "- Images are RGB photographs of rice leaves at various scales.\n",
    "- Background varies: white paper, soil, field, close-up.\n",
    "-  images show water-soaked lesions often confused with  brown lesions.\n",
    "-  shows distinctive rolled-leaf morphology, easy to distinguish visually.\n",
    "\n",
    "These observations informed the augmentation choice: moderate  (not extreme,\n",
    "as colour is a diagnostic feature for distinguishing diseases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ec75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12,983 images in 7 classes\n",
      "Test : 2,799 images in 7 classes\n",
      "Class-to-idx: {\"blast\": 0, \"healthy\": 1, \"insect\": 2, \"leaf_folder\": 3, \"scald\": 4, \"stripes\": 5, \"tungro\": 6}\n",
      "\n",
      "train: blast:3601, healthy:3229, insect:1654, leaf_folder:1332, scald:294, stripes:1458, tungro:1415\n",
      "test: blast:775, healthy:694, insect:357, leaf_folder:288, scald:64, stripes:315, tungro:306\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets as tvd\n",
    "train_ds=tvd.ImageFolder(\"../riceleaf/train\",transform=tl_train_tf)\n",
    "test_ds=tvd.ImageFolder(\"../riceleaf/test\",transform=tl_val_tf)\n",
    "\n",
    "print(f\"Train: {len(train_ds):,} images in {len(train_ds.classes)} classes\")\n",
    "print(f\"Test : {len(test_ds):,} images in {len(test_ds.classes)} classes\")\n",
    "print(f\"Class-to-idx: {train_ds.class_to_idx}\")\n",
    "print()\n",
    "# Sample count per split per class\n",
    "for split,ds in [(\"train\",train_ds),(\"test\",test_ds)]:\n",
    "    cnt={}\n",
    "    for _,lbl in ds.samples: cnt[lbl]=cnt.get(lbl,0)+1\n",
    "    print(f\"{split}: \"+\", \".join(f\"{CLASSES[k]}:{v}\" for k,v in sorted(cnt.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e41e8",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Differences from Deep Learning Pipeline\n",
    "\n",
    "| Aspect | DL (Custom CNN) | TL (EfficientNet-B4) |\n",
    "|---|---|---|\n",
    "| Input size | 224x224 | **380x380** |\n",
    "| Normalisation | Dataset stats | **ImageNet stats** |\n",
    "| Augmentation strength | Strong | Moderate |\n",
    "| Batch size | 32 | **16** |\n",
    "| Pretrained | No | **Yes (ImageNet1K_V1)** |\n",
    "\n",
    "These differences are deliberate and necessary for correct fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_stats_tl.json saved.\n",
      "Next: 02_training_compare.ipynb\n"
     ]
    }
   ],
   "source": [
    "import json,pathlib as pl\n",
    "stats={\n",
    "    \"imagenet_mean\":[0.485,0.456,0.406],\n",
    "    \"imagenet_std\":[0.229,0.224,0.225],\n",
    "    \"input_size_tl\":380,\n",
    "    \"batch_size\":16,\n",
    "    \"train_counts\":{\"blast\":3601,\"healthy\":3229,\"insect\":1654,\"leaf_folder\":1332,\"scald\":294,\"stripes\":1458,\"tungro\":1415},\n",
    "    \"test_counts\":{\"blast\":775,\"healthy\":694,\"insect\":357,\"leaf_folder\":288,\"scald\":64,\"stripes\":315,\"tungro\":306}\n",
    "}\n",
    "with open(pl.Path(\".\")/\"dataset_stats_tl.json\",\"w\") as f:\n",
    "    json.dump(stats,f,indent=2)\n",
    "print(\"dataset_stats_tl.json saved.\")\n",
    "print(\"Next: 02_training_compare.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
